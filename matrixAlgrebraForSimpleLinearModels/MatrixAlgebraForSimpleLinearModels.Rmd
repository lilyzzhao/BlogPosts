---
title: "Matrix Algebra For Simple Linear Models"
output: html_notebook
---

Linear regression is a fundamental statistics and machine learning technique. *Yes*, you read that right. Linear regression falls under 'machine learning.' So get on your fancy pants, we're about to do some smart shit!

Simple linear regression easily expands to multiple linear regression, generalized linear models, hierarchical models, the list goes on. Because it's so fundamental to our understanding of more complicated models, 
you'll want to know it inside and out. But to really understand how it generalizes to other models, we're going to need to learn about the matrix algebra that's going on under the hood. *Ugh, matrices again?* Yes, you can NEVER escape them, MWAHAHA!!! So, let's lift up the hood and take a look. We'll start by simulating a toy example for us to work with.

# Simulating meow-riffic data

Let's start by simulating a positive relationship between age and the number of cats a person has. 

```{r}
# Load libraries.
library(plotly)
library(tidyverse)
```

```{r}
# Simulated a vector representing ages between 15 and 60.
age <- seq(15, 60, by = 1)
# Simualate error to go along with our linear model. 
set.seed(3)
error <- rnorm(46, 0, 1)
# Simulate a linear model for the number of cats as a function of age.
cats = round(.2 * age + 4 + error)
# Combine age and cats into a dataframe. 
catsDF <- data.frame(cats, age)
# Subset the data to 10 data points for easy matrix representation. 
catsData <- sample_n(catsDF, 10)
# Plot data.
plot_ly(data = catsData, x = ~age, y = ~cats, type = "scatter", mode = "markers")
```

# The math-y part of the simple linear model

Simple linear regression models the relationship between a dependent variable (cats) and an explanatory variable (age). You can think of it as, the number of cats we have *depends* on our age. Simple linear regression takes the form 

$$y = \beta_0 + \beta_1x + \epsilon$$

with $\beta_0$ and $\beta_1$ representing unknown model parameters which we'll want to estimate.  

$\beta_0$ is generally referred to as the **intercept** and $\beta_1$ as the **slope** of the linear relationship. We want to find $\beta$ values, or coefficients, that minimize the error term of the equation, $\epsilon$. The smaller our error terms, the closer the data fits our linear relationship. 

We can write the linear model equation in terms of the observations:

$$y_i = \beta_0 + \beta_1x_i, i = 1, 2, ...n$$

In our cats dataframe, we have n = 10 data observations consisting of an x (age) and y (cats) value. For our first observation, $x_1$ = 35 years old and $y_1$ = 12.54 cats.

This can be re-written in matrix notation as:

$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$

where

$\mathbf{y} = \begin{bmatrix}
16 \\
38 \\
21 \\
24 \\
52 \\
33 \\
47 \\
57 \\
60 \\
25
\end{bmatrix}$,

$\mathbf{x} = \begin{bmatrix}
1& 7 \\
1& 10 \\
1& 8 \\
1& 10 \\
1& 14 \\
1& 12 \\
1& 14 \\
1& 17 \\
1& 14 \\
1& 8
\end{bmatrix}$,

$\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}$,

and $\boldsymbol{\epsilon} = \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\epsilon_4 \\
\epsilon_5 \\
\epsilon_6 \\
\epsilon_7 \\
\epsilon_8 \\
\epsilon_9 \\
\epsilon_{10}
\end{bmatrix}$

```{r}
# Define design matrix. 
X <- cbind(x0 = rep(1, nrow(catsData)),
           x1 = catsData$age)
# Define observed dependent variable. 
y <- catsData$cats
# Calculate and define X'X matrix.
`X'X` <- t(X) %*% X
# Calculate and define X'y matrix.
`X'y` <- t(X) %*% y
```

```{r}
# Calculate and define estimated beta coefficients.
b <- solve(`X'X`, `X'y`)
```



```{r}
# Fit a linear model using lm.
fit <- lm(cats ~ age, data = catsData)
# Print summary of linear model.
summary(fit)
```