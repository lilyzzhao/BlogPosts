---
title: "'Simple' Linear Regression"
output: html_notebook
---

Simple linear regression models the relationship between a dependent variable and an explanatory variable. For example, let's take a very real and 100% legitimate data set (honestly no need to Google it) involving the number of cats a person should have based on their age. We'll simulate this data, not because the data is fabricated, but for...learning purposes. 

# Simulating meow-riffic data

```{r, message=FALSE, warning=FALSE}
# Load libraries.
library(plotly)
library(tidyverse)
```

```{r}
# Simulated a vector representing ages between 15 and 60.
age <- seq(15, 60, by = 1)
# Simualate error to go along with our linear model. 
set.seed(3)
error <- rnorm(46, 0, 1)
# Simulate a linear model for the number of cats as a function of age.
cats = round(4 + 0.2 * age + error)
# Combine age and cats into a dataframe. 
catsDF <- data.frame(cats, age)
# Subset the data to 10 data points for easy matrix representation. 
catsData <- sample_n(catsDF, 10)
# Plot data.
plot_ly(data = catsData, x = ~age, y = ~cats, type = "scatter", mode = "markers")
```

Now let's work on fitting a line that best 'describes' the relationship we see.

# Simple linear regression 

Simple linear regression models the relationship between a dependent variable (cats) and an explanatory variable (age). You can think of it as, the number of cats we have *depends* on our age. Simple linear regression takes the form: 

$$y = \beta_0 + \beta_1x + \epsilon$$

with $\beta_0$ and $\beta_1$ representing unknown model parameters which we'll want to estimate. Because we've simulated our data, we know $\beta_0 = 4$ and $\beta_1 = 0.2$. 

$\beta_0$ is generally referred to as the **intercept** and $\beta_1$ as the **slope** of the linear relationship. The intercept tells us where the line crosses the y-axis and the slope tells us how many units up or down we move for every unit increase in x.

We want to find $\beta$ values that minimize the error term of the equation, $\epsilon$, with the method of least squares. The smaller our error term, the closer the data is to the linear relationship. 

*But, we have multiple y's and multiple x's.* Nice catch! We can write the linear model equation in terms of the observations:

$$y_i = \beta_0 + \beta_1x_i + \epsilon_i, i = 1, 2, ...n$$ 

With the method of least squares, we'll want to minimize the errors by minimizing

$$S = \sum_{i=1}^{n} \epsilon_i^{2} = \sum_{i=1}^{n} (y_i - \beta_0-\beta_1x_i)^{2}$$

*Note that $\epsilon_i = y_i - \beta_0 - \beta_1x_i$ when you re-write the linear model, solving for $\epsilon_i$.

# Finding the least squares solution in R

By minimizing the sum of squares of the residuals, we will get estimates of our $\beta$ parameters. But instead of performing these calculations by hand, we can have R do this for us using the *lm( )* function. The *lm( )* function takes a formula (y ~ x) and a dataframe. 

```{r}
# Fit a linear model using lm.
fit <- lm(cats ~ age, data = catsData)
# Print summary of linear model.
summary(fit)
```

We find our $\beta$ estimates, **b**, in the estimates column of the coefficients section. The first value, labeled '(Intercept),' gives us our estimate for $\beta_0$, b_0. Our second value, labeled 'age,' gives us our estimate for $\beta_1$, b_1. 

Did we recover our simulated values of $\beta_0 = 4$ and $\beta_1 = 0.2$? Not perfectly, but our estimates are close. In this example, we had a sample size of 10. As our sample size approaches infinity, our estimates should approach our parameter values. 

# Plotting our linear model, or meow-del. 

Let's use our estimates to plot our line of best fit. To do this, we'll use the fitted() function. Fitted will give us a prediction for the number of cats we expect a person to have based on the age from our catsData dataframe given our model. 

```{r}
plot_ly(data = catsData, x = ~age, y = ~cats, type = "scatter", mode = "markers") %>%
  add_trace(x = ~age, y = fitted(fit), mode = "lines") %>%
  layout(showlegend = FALSE)
```


# Predicting values based on linear model















